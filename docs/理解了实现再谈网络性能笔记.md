# 理解了实现再谈网络性能笔记

## 内核如何接收网络包

在开始收包之前，LInux的准备工作：

* 创建`ksoftirqd`线程，设置线程函数（处理软中断）
* 协议栈注册（arp、icmp、ip、udp、tcp），注册各自协议的处理函数，便于在数据包来临之后快速找到对应的处理函数
* 网卡驱动初始化，每个驱动都有一个初始化函数，在此过程中，每个驱动自己的DMA准备好，NAPI的poll函数地址告诉内核
* 启动网卡，分配RX、TX队列，注册中断对应的处理函数

数据来临，网卡处理：

* 网卡将数据帧DMA到内存的RingBuffer中，然后向CPU发起中断通知
* CPU响应中断请求，调用网卡启动时注册的中断处理函数（硬中断）
* 硬中断操作简单，记录了一个寄存器，修改了一下CPU的poll_list，发出软中断
* 内核线程ksoftirqd线程发现软中断请求，先关闭所有硬中断
* ksoftirqd线程调用驱动poll函数收包
* poll函数将收到的包送到协议栈注册`ip_rcv`函数
* `ip_rcv`函数将包送到`udp_rcv`函数或者`tcp_rcv`函数





## 内核与进程协作

### `udp`协议处理

* `udp_tcv`函数中调用`__upd4_lib_rcv`函数，其中的`__udp4_lib_lookup_skb`函数根据skb寻找对应的socket，找到之后将数据包放到socket缓存队列；如果没有找到，发送一个目标不可到达的`icmp`包
* 判断是否在此socket上有系统调用，有则将数据包添加到`backlog`队列；没有则将将数据包直接放在socket接收队列中；当用户释放socket时，内核会检查`backlog`队列，如果有数据就移动到接收队列中（接收队列如果满了，直接将包丢弃）
* `recvfrom`系统调用实现：
  * `recvfrom`是一个`glibc`库函数，执行之后会从用户陷入内核态，进入`sys_recvfrom`
  * socket数据结构中的`const struct proto_ops`对应的是协议的方法集合，对于`IPv4 Internet`来说，`udp`是通过`inet_dgranm_ops`定义的，其中注册了`inet_recvmsg`方法；
  * socket数据结构中的另一个数据结构`struct sock* sk`，其中的`sk_prot`定义了二级处理函数，对于`udp`，设置成`udp_prot`方法集
  * `inet_recvmsg`调用`sk->sk_prot->recvmsg`，进入到`udp_recvmsg`函数，再进入到`__skb_recv_datagram`函数，其中访问了`sk->sk_receive_queue`，如果没有数据且用户允许等待，则调用`wait_for_more_packets()`等待（让用户进入睡眠（睡眠状态与`tcp`的处理一致））





### `tcp`协议处理

**同步阻塞网络IO是高性能网络开发路上的绊脚石！**

* 创建一系列的socket（其中之一）：`sock_create()`->`__sock_create()`

  * `sock_alloc()`分配sock对象->获取协议族操作函数表，调用其`create()`（对于`AF_INET`来说，执行`inet_create()`）
  * 在`inet_create()`根据`SOCK_STREAM`找到对应的`tcp`对应的`inet_stream_ops`和`tcp_prot`，分别设置到`socket->ops`和`sock->sk_prot`，调用`sock_init_data()`
  * `sock_init_data()`将sock中的`sk_data_ready`函数指针进行初始化，默认设置为`sock_def_readable()`（**当软中断上收到数据包时会通过调用`sk_data_ready`函数指针（实际上是`sock_def_readable()`）唤醒sock上的等待的进程**）

* 等待接收数据：

  * `recv`（`clib`库函数）->`recvfrom`系统调用（用户态->内核态）

  * 调用socket对象`ops`里的`recvmsg`（由上述：`inet_recvmsg`），其中调用`sk_prot`里的`recvmsg`（由上述：`tcp_recvmsg`）

  * 遍历接收队列接受的数据：没有收到足够的数据，启用`sk_wait_data`阻塞当前进程；该函数访问的是sock对象下的接收队列`sk->sk_receive_queue`

  * `sk_wait_data()`：

    * 当前进程关联到定义的等待队列（宏`DEFINE_WAIT(wait)`）
    * 调用`sk_sleep`获取sock对象下的wait并准备挂起，将进程状态设置成`INTERRUPTIBLE`（获取sock对象下的等待队列列表头`wait_queue_head_t`）；调用`prepare_to_wait`把新定义的等待队列项wait插入到sock的等待队列下
    * 通过调用`sk_wait_event()`(`schedule_timeout`)让出CPU，然后进入睡眠

    `DEFINE_WAIT(wait)`：定义了一个等待队列项wait，注册回调函数`autoremove_wake_function`，把当前进程描述符`current`关联到`.private`成员上；(**导致一次进程上下文切换开销**)

* 软中断处理模块

  * `tcp_v4_rcv`根据接收到的网络包的`header`里的信息查询对应的socket，之后进入`tcp_v4_do_rcv`
  * 假设处理的是`ESTABLISH`状态下的包，进入`tcp_rcv_established()`
  * 调用`tcp_queue_rcv()`，将接收数据放到socket的接收队列上
  * 调用`sk_data_ready()`唤醒socket上的等待用户进程（函数指针：实际上是调用`sock_def_readable()`）
  * `sock_def_readable()`：访问`sock->sk_wq`的wait，调用`wake_up_interruptible_sync_poll`唤醒socket上因为等待数据被阻塞掉的进程
  * `__wake_up_common`实现唤醒：其中的`nr_exclusive`为1（**这里指的是即使有多个线程阻塞在同一个socket上，也就只唤醒一个进程，作用是防止惊群**）；找出一个等待队列项`curr`，调用`curr->func`（即上述的`autoremove_wake_function()`），调用`default_wake_function()`，调用`try_to_wake_up()`，传入`task_struct`为`curr->private`，就是**当时因为等待而被阻塞的进程项，此时socket上等待而被阻塞的进程被推入可运行队列中，导致又一次进程上下文开销**

### `tcp`和`udp`处理上的异同

根据以上的知识点进行总结（疑点待解决）

* socket：

  `udp`是根据函数调用里的`skb`找到对应的`socket`的，如果没有找到，就发送回去一个`icmp`；`tcp`是通过创建和初始化一个对应的`socket`的

* `sk->ops`和`sk->sk_prot`设置时机：

  `udp`是根据既有的`socket`*（一开始udp的socket是accept创建的）*；`tcp`是在创建`socket`初始化的时候设置的*（tcp的socket也是在accept时候创建的）*

* 睡眠处理函数：

  一致

* 都是`sk->ops`里的`inet_recvmsg`调用`sk->prot`里的`recvmsg`

* 等待接收数据，都是陷入内核态调用系统的`recvfrom`函数

* 进入到各自的二级处理函数`recvmsg`之后都会访问接收队列，没有数据都是阻塞（阻塞模式下）







## IO多路复用--epoll

* `accept`创建新的`socket`

  创建一个新的`socket`并初始化（`sk->ops`和`sk->prot`，与监听的`socket`一致，直接复制过来，为新的`socket`申请`file`（也有一个`socket`指针），`file->f_op->poll`函数指向`sock->poll`）

* 接受连接：初始化`sock`

  `sock`对象的`sk_data_ready`设置成`sk_def_readable`

* 添加新文件到当前进程打开文件列表

`epoll`的实现机制：[epoll实现机制](https://www.zjp7071.cn/epoll/)





## Linux内核发送数据包

总览图：

![all](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/Linux%E7%BD%91%E7%BB%9C%E5%8F%91%E9%80%81%E6%80%BB%E8%A7%88.drawio.png)



### 网卡启动准备

网卡一般支持多个队列，每一个队列用`RingBuffer`表示

* 调用`__igb_open()`函数分配和初始化`RingBuffer`

  * 分配所有**传输队列**（TX）
  * 分配所有**接收队列**（RX）
  * 开启全部队列

  关注传输队列（TX），分配初始化过程中，申请了两个环形数组：

  * 一个用于内核使用
  * 一个用于网卡硬件使用（可通过DMA访问到）

  两个环形数组用一个指针指向同一个`skb`，这样内核就可以共同`RingBuffer`访问数据了

### accept创建新的socket

详见上述创建过程

### 发送数据开始

#### send系统调用

![send_sendto](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8.drawio.png)

#### 传输层处理

##### 传输层拷贝

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send_%E4%BC%A0%E8%BE%93%E5%B1%82%E6%8B%B7%E8%B4%9D.drawio.png)

##### 传输层发送

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send_%E4%BC%A0%E8%BE%93%E5%B1%82%E5%8F%91%E9%80%81.drawio.png)

*(4.设置`tcp`头)*

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/skb%E6%8C%87%E9%92%88%E7%A7%BB%E5%8A%A8.jpg)



#### 网络层发送处理

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send_%E7%BD%91%E7%BB%9C%E5%B1%82%E5%8F%91%E9%80%81%E5%A4%84%E7%90%86.drawio.png)

#### 邻居子系统

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send_%E9%82%BB%E5%B1%85%E5%AD%90%E7%B3%BB%E7%BB%9F.drawio.png)

#### 网络设备子系统

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send_%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%AD%90%E7%B3%BB%E7%BB%9F.drawio.png)

#### 软中断调度

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send_%E8%BD%AF%E4%B8%AD%E6%96%AD%E8%B0%83%E5%BA%A6.drawio.png)



### `igb`网卡驱动发送

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send_igb%E7%BD%91%E5%8D%A1%E9%A9%B1%E5%8A%A8%E5%8F%91%E9%80%81.drawio.png)



### 发送完成硬中断

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send_%E5%8F%91%E9%80%81%E5%AE%8C%E6%88%90%E7%A1%AC%E4%B8%AD%E6%96%AD.drawio.png)





### 总结

![](https://myblog-1308923350.cos.ap-guangzhou.myqcloud.com/img/send%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E6%80%BB%E7%BB%93%E5%9B%BE.jpg)

### 问题

1. 我们在监控内核发送数据消耗的 CPU 时，是应该看 `sy `还是 `si `？

   在⽹络包的发送过程中，⽤户进程（在内核态）完成了绝⼤部分的⼯作，甚⾄连调⽤驱动的
   事情都⼲了。 只有当内核态进程被切⾛前才会发起软中断。 发送过程中，绝⼤部分（90%）
   以上的开销都是在**⽤户进程内核态**消耗掉的。
   只有⼀少部分情况下才会触发软中断（NET_TX 类型），由软中断 `ksoftirqd `内核进程来发
   送。
   所以，在监控⽹络 IO 对服务器造成的 CPU 开销的时候，不能仅仅只看 `si`，⽽是应该把 `si`、`sy`

2. 在服务器上查看 `/proc/softirqs`，为什么 `NET_RX `要⽐ `NET_TX` ⼤的多的多？

   第⼀个原因是当数据发送完成以后，通过硬中断的⽅式来通知驱动发送完毕。但是**硬中断⽆
   论是有数据接收，还是对于发送完毕，触发的软中断都是 NET_RX_SOFTIRQ**，⽽并不是
   NET_TX_SOFTIRQ。

   第⼆个原因是对于**读**来说，都是要经过 **NET_RX 软中断**的，都⾛ **`ksoftirqd `内核进程**。⽽对于
   **发送**来说，绝⼤部分⼯作都是在**⽤户进程内核态**处理了，**只有系统态配额⽤尽才会发出
   NET_TX**，让软中断上。

3. 发送⽹络数据的时候都涉及到哪些内存拷⻉操作？

   这⾥的内存拷⻉，我们只特指**待发送数据的内存拷⻉**。

   第⼀次拷⻉操作是内核申请完 `skb` 之后，这时候会将⽤户传递进来的 buffer ⾥的数据内容都
   拷⻉到 `skb` 中。如果要发送的数据量⽐较⼤的话，这个拷⻉操作开销还是不⼩的。

   第⼆次拷⻉操作是从传输层进⼊⽹络层的时候，每⼀个 `skb` 都会被克隆⼀个新的副本出来。
   ⽹络层以及下⾯的驱动、软中断等组件在发送完成的时候会将这个副本删除。**传输层保存着
   原始的 `skb`，在当⽹络对⽅没有 ack 的时候，还可以重新发送，以实现 TCP 中要求的可靠传
   输**。

   第三次拷⻉不是必须的，只有当 IP 层**发现 `skb `⼤于 MTU 时**才需要进⾏。会再申请额外的
   `skb`，并将原来的` skb` 拷⻉为多个⼩的 `skb`。



> 这⾥插⼊个题外话，⼤家在⽹络性能优化中经常听到的零拷⻉，我觉得这有点点夸张的
> 成分。TCP 为了保证可靠性，第⼆次的拷⻉根本就没法省。如果包再⼤于 MTU 的话，分
> ⽚时的拷⻉同样也避免不了。





---







